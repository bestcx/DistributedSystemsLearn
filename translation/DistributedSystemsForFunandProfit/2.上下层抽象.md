# 分布式系统二

## 上下层的抽象

​		在本章节,我们会浏览上下层的抽象,了解一些不可能性结论(CAP,FLP). 并且了解怎么实现性能的目标
(sake)而选择.

​		如果你已经从事过编程,那你肯定对各层次抽象的想法更加熟悉,你可能在一些抽象层上进行工作,或者是通过Api接口与更底层的进行操作.你也可能会提供更高层次的Api和用户接口给使用者. 计算机网络的七层通讯协议就是一个很好的例子.

​		分布式编程就是在处理由于使用分布式而产生的一大部分的后果.这是一个趋势在我们用于许多的结点和我们希望系统可以像一台机器那样的方便.这就意味着我们需要找到一个能够平衡这个很难理解的层面和操作层面的一个抽血层.

​		什么就意味着我们说 X 比 Y 更加抽象呢? 首先, X 不是在 Y 的基础上去介绍任何新的东西或者是根本上的不同. 事实上,  X 可能移除了一些 Y 的影响或者说是在某种程度下变得更加好管理了. 第二,  在某种观念下 X 比 Y 更容易去控制(grasp).假设 X 从 Y 上移除掉的东西对我们来说是不重要的.

​		抽象,从根本上来说就是虚假的. 所有的条件的都是唯一的,就像是每个结点一样. 但是抽象让这个世界更加可控:  简单的问题说明是更加容易去分析(analytically)管理(tractable)的, 并且可以给我们提供我们无法忽视的重要的东西, 解决方案也是更加广泛可适用的.

​		事实上,如果我们保留(keep around)的东西是非常重要的 ,结果就可以被大规模的应用. 这也是为什么不可能性结论是非常重要的,他关于问题提供了一个简单的可实现的构想, 并证明在某些约束或假设下是不可能解决的.

​		所有抽象都会忽视一些东西, 以便于将现实中独一无二的东西等同起来. 使用技巧将那些不重要的东西都摆脱掉. 你不可能提前(a priori)知道那些东西是不重要的.

​		每次,我们从系统规范中排除掉某些东西的时候, 我们就会有风险引入一些错误源和影响性能的问题. 这也是为什么有时候从其他方向,或者有选择的引入硬件的某些方面和现实中的问题.他也代表着他代表我们会完全的(sufficient)再引入(reintroduce)一些特殊的硬件特性(例如 物理存储顺序)和物理特性来让系统的性能足够的好.	

​		在这种思想下,我们保留最少真实量是多少,才能正常工作在一个可辨认的(recognizable)分布式系统中,一个系统模型就是我们需要考虑的特性的说明书.我们选择一个后,就可以去看看不可能性的结果和挑战.

##  系统模型

​		分布式系统的一个关键属性是分布. 更具体的说(More specifically), 编写一个分布式的程序:

- 同时的(concurrently)运行在多个独立的结点上

- 通过网络来进行连接,这势必会引入不确定性(nondeterminism)和消息丢失

- 没有共享内存和共享时钟

  ​	

​       这是一些隐含的寓意:

- 每个结点在同时执行程序
- 结点能够快速访问本地的状态,但是任何全局的状态都可能(potentially)会过期.
- 结点的操作可以操作并且可以独立的从错误中恢复操作.
- 信息可能会延误或者是丢失(独立的结点错误,是很难辨认(distinguish)出失败是由于网络的原因还是结点的原因)
- 时钟是不能在结点间同步访问的(本地的时间戳与全局的实时时间不一致(correspond),他是很难察觉到的)

​		一个系统模型例举出许多与单独的系统设置相关联的设想.

> 系统模型
>
> 一组关于实现分布式系统所依赖的环境和设备的假设

​		系统模型对环境和设备的假设各不相同.这写假设包括以下:

- 结点性能怎么样,他们怎么会失败
- 通讯链路是怎么运作的,他们怎么会失败
- 整个系统的属性,例对于时序的假设

​         一个健壮的系统模型只会做出一些很弱的假设: 任何为这个系统编写的算法能够容忍不同的环境,因为他只需要做很少很弱的假设.

​		另一方面, 我们可以自己建立这样一个系统模型,他需要很强的假设才能去简单的推理. 例如,假设结点没有挂掉这意味着我们的算法不会处理结点挂掉.但是,像这样的系统模型是不切实际并且因此很难应用到实践.

​		让我们更详细地看看结点、链接、时间和顺序的属性。

## 系统模型中的结点

​		结点是为计算和存储服务的主机.

- 他们有能力去执行一个程序
- 有能力存储数据到非永久性内存(失败会导致数据丢失)和稳定的存储(失败后还可以读取)
- 时钟(被认为是准确的时钟)

​		结点执行确定的算法: 本地计算,本地计算后的状态和被发送的消息由接收到的信息和接收到信息的本地状态唯一决定.

​		这里有很多可能错误的模型,他们描述了结点失败的方式. 实际上, 大多数系统都采用崩溃-恢复故障模型: 结点只能在崩溃时挂掉,并且可以在不久后的时间点从崩溃进行恢复.

​		另外一种可选择的(alternative)假设是结点可能因为随意的不正确的行为导致挂掉. 就像是拜占庭错误容忍. 拜占庭错误很少的在现实中的商业系统中发生. 因为对任意错误具有弹性的算法运行成本更高，实现起来也更复杂。

## 系统模型中的通讯链路

​		通讯链路连接着互相独立的结点,并且允许信息被发送到任意的位置.  许多讨论分布式算法的书都这样假设,每一对结点上都有独立的链路, 这些链接中的消息都是安装先进先出的顺序,他们只能传递已发送的消息, 并且这些消息可能会丢失.	

​		一些算法假设网络是值得信赖的, 消息从不会丢失并且不会有不确定的延迟. 对于现实生活的设置中这个假设是合理的,但一般来说,我们最好还是考虑到网络的不可用和数据的丢失和延误.

​		当网络断掉的同时结点自身还在操作就会发生网络分裂的问题. 当他发生以后, 消息就可能会丢失和延误知道网络分裂被修复. 分裂的结点可以被一些客户端进行访问. 崩溃掉的结点就应该是区别对待了. 下面的图描述了结点故障和网络分离.

![replication](http://book.mixu.net/distsys/images/system-of-2.png)

​		很少有对网络连接更远的假设. 我们可以假设链接仅在一个方向上起作用，或者我们可以为不同的链接引入不同的通信成本（例如，由于物理距离导致的延迟）。但是，除了长链接（WAN延迟）以外，在商业环境中很少考虑这些问题，因此在此不做讨论。 成本和拓扑的更详细模型可以以复杂性为代价进行更好的优化。

## 时序假设

​			物理分布的其中一个结果(consequences)就是每个结点都有自己独特的经历. 这是不可避免的, 信息仅仅可以以光速进行传播. 如果结点间的距离不同, 那么信息从一个结点到达另外的结点上的时间也会不同,并且到达结点的顺序也有可能是不同的.时间假设是一种方便的速记方式，用来捕捉关于我们考虑这一现实的程度的假设。两种主要的选择是:

> **Synchronous system model**
>
> ​	进程以锁步进行执行, 消息的传输(transmission)延迟有一个明显的上界, 每个进程都有精确的(accurate)时钟
>
> **Asynchronous system model**
>
> ​	没有对时间的假设. 进程以独立的频率进行运行,消息的传输延迟也没有界限, 时钟也不起作用.

​		同步系统模型在时间和顺序上指定了许多约束.他从本质上假设结点有同样的经历: 被发送的消息总会在确定的最大延时接收到.并且程序是以锁步执行的. 他很方便,因为我们可以对时间和顺序进行假设而异步系统模型却不行.	

​		异步则是不能假设的,对于他来说就是我们不能去依赖时间(或者是对时间敏感).

​		在同步系统模型中是很容易解决问题的, 执行的速度,最大的消息传送延时和准确的时钟都在帮助我们更简单的解决问题,因为我们可以根据这些假设做出推断(make inferences based on)，并通过假设它们永远不会发生来排除不方便(inconvenient)的故障场景(scenarios)。

​		当然，假设同步系统模型并不特别现实。现实世界的网络容易出现故障，并且没有消息延迟的硬性限制。现实世界的系统至多是部分同步的:它们可能偶尔能正确工作并提供一些上限，但有时会出现消息无限期延迟和时钟不同步的情况。

## 一致性问题

​		在本文的其余部分中，我们将改变系统模型的参数。接下来，我们将看看如何改变两个系统的属性:

- 网络分裂是否包含在错误模型中
- 同步和异步的时间假设

​		通过讨论俩个不可能性结果((FLP and CAP))来探讨影响系统设计的选择. 为了进行讨论,我们需要引入一个问题来解决.我们将讨论一致性问题.

​		几台计算机或者是结点如果他们在某些值达成一致,那他们就可以达成共识,准确来说:

   			1. 	共识: 每一个正确的进程都能在某些值上达成一致
   			2. 	完整(Integrity):每个正确的进程最多决定一个值，如果它决定某个值，那么它一定是由某个进程提出的。
   			3. 	(最终)Termination:  所有的进程最终达成一个决定
   			4. 	正确性(Validity): 如果所有正确的进程提出(propose)相同的值V，那么所有正确的进程决定V

​		一致性问题是许多商业分布式系统的核心,毕竟我们希望分布式系统的可靠性和性能不需要处理分布的后果(结点间的分歧和不一致),解决一致性问题可以解决几个相关的、更高级的问题，如原子广播和原子提交。

## 俩个不可能性结论

​		一个是FLP理论,主要是面向设计分布式算法的人,另一个是CAP理论,主要是面向开发者,需要在不同的系统模型中做选择,并且不关心算法的设计.

### FLP理论

​		这里只会简单的说明FLP理论,它主要在学术圈被深入研究. FLP理论主要测试了在异步系统模型下的一致性问题(从技术上来讲共识问题时一种很弱的一致性问题). 他假设结点只能通过崩溃来失败;网络是可靠的,并且异步系统模型的典型定时假设是成立的:例如,没有消息延迟的限制。

​		在这些假设下，FLP结果表明“在一个容易失败的异步系统中不存在一致性问题(确定性)算法，即使消息永远不会丢失，最多一个进程可能失败，它只能通过崩溃(停止执行)来失败”。

​		这个结果意味着在一个非常小的系统模型下，不可能以一种不会永远延迟的方式来解决一致性问题。争论是，如果存在这样一个算法，那么可以设计一个算法的执行，通过延迟消息传递，在任意时间内它将保持未决定(“俩个值”)——这在异步系统模型中是允许的。因此这样的算法是不可能存在的.  这个不可能性结论很重要，因为它强调了假设异步系统模型会导致一个权衡:当关于消息传递边界的保证无效时，解决一致性问题的算法必须要么放弃安全性，要么放弃生命力。

​		这一见解与设计算法的人特别相关，因为它对我们知道在异步系统模型中可以解决的问题施加了严格的约束。CAP定理是一个与实践者更相关的相关定理:它做出了略微不同的假设(网络故障而不是结点故障)，并且对实践者在系统设计之间的选择有更明确的含义。

### CAP理论

​		CAP理论是计算机科学家Eric Brewer最先(initially)提出的一个猜想(conjecture).他是一种非常受欢迎和有用的方式来考虑系统设计时保持一致性的权衡.它甚至有吉尔伯特和林奇的正式证明，不，Nathan Marz没有揭穿它，尽管有一个特别的讨论网站认为。

​		这个理论阐述了以下三个点:

- 一致性: 所有的结点在同一时间可以看到一样的数据
- 可用性: 结点失败了不会阻止幸存者继续操作
- 分区容忍: 由于网络或者其他结点挂掉不会导致信息丢失系统任然会继续运行

​		只有俩点可以同时被满足. 看下图: 

![CAP theorem](http://book.mixu.net/distsys/images/CAP.png)

​		我们可以将其总结为以下三个内容: 	

- CA (consistency + availability). 该应用包括有完全严格的仲裁协议, 例如俩阶段提交
- CP (consistency + partition tolerance). 该应用包括少数分区不可用的多数仲裁协议.例如Paxos
- AP (availability + partition tolerance). 该应用使用冲突解决的协议，如Dynamo。





​		CA和CP进行系统设计都提供了相同的一致性模型:强一致性. 他们之间的区别就是CA系统不能容忍任何的结点挂掉.在非拜占庭故障模型中，给定2f+1个结点，CP系统可以容忍至多f个故障(简而言之就是,只要有2f+1个结点活着就可以容忍f个结点挂掉),原因很简单:

- CA不能区别是结点挂掉还是网络挂点. 因此必须停止写操作来避免引入差异.	它不能判断是远程节点宕机了，还是仅仅是网络连接宕机了:所以唯一安全的方法是停止接受写入。
- CP系统通过强制分区两边的不对称行为来防止分歧(例如保持单副本的一致性)。它只保留了多数分区，并要求少数分区变得不可用(例如停止接受写)，这保留了一定程度的可用性(多数分区)，并仍然确保单副本一致性。

​		当我讨论Paxos时，我将在关于复制的章节中更详细地讨论这个问题。重要的是CP系统将网络分裂合并(incorporate)到其故障模型中，并使用Paxos、Raft或viewstamped等算法区分多数分区和少数分区。CA系统不是分区感知的，:它们经常使用两阶段提交算法，并且在传统的分布式关系数据库中很常见。

​		假设出现了分割，该定理就变成了可用性和一致性之间的二选一。

![Based on http://blog.mikiobraun.de/2013/03/misconceptions-about-cap-theorem.html](http://book.mixu.net/distsys/images/CAP_choice.png)

​		我认为从CAP定理可以得出四个结论(conclusions):

​		首先,早期的分布式关数据库的设计在设计时并没有考虑到分区容忍. 分区容忍是现代系统中一个重要的属性,因为现在的系统大多数是地理上的分布式,那么网络分裂的现象会变得更多.

​		第二,	当发生网络分裂时, 在强一致性和高可用之间就可以进行一种权衡. CAP理论就是说明了在强保证和分布式计算时的权衡.

​		从某种意义上说，承诺一个由不可预测的(unpredictable)网络连接的独立节点组成的分布式系统“以一种与非分布式系统无法区分的方式运行”是相当疯狂的。	

![From the Simpsons episode Trash of the Titans](http://book.mixu.net/distsys/images/news_120.jpg)

​		强一致性保证要求我们在分区的时候放弃可用性.这是因为在继续接受分区两边写操作的同时，无法防止两个无法相互通信的副本之间的分歧。

​		那我们该怎么去解决这个问题,我们在更深一步的假设(假设没有分区)和弱化这个保证. 一致性可以与可用性(以及相关的离线可访问性和低延迟能力)相权衡。如果这个一致性定义的程度小于"所有节点在同一时间看到相同的数据",那么我们可以同时保证可用性和一些(较弱的)一致性。

​		第三, 在常规操作下强一致性和性能也可以进行一定的权衡.

​		强一致性和单副本一致性需要节点进行交流,并且对所有操作达成一致. 这就会导致在正常操作下的高延迟.

​		如果我们使用不是经典的那种一致性模型,就是允许副本延迟或是有差异的一致性模型, 那么我们就可以在正常操作期间减少延迟, 并在分区存在时保持可用性。

​		当我们的操作涉及到越少的信息传递和节点时, 一个操作完成的就越快.但是实现这个目标的唯一途径就是放松保证的程度: 减少一些节点的接触频率，这意味着节点可能包含旧数据。

​		这也可能会导致异常的发生, 你不在去保证拿到最新的数据.根据所做的保证的类型，您可能会读取比预期更早的值，甚至丢失一些更新。

​		第四, 是一种间接的,如果我们不想在网络分区期间放弃可用性，那么我们需要探索除了强一致性之外的一致性模型对我们的目的是否可行。

> 网络分裂和网络分区一样
>
> 一个分布式系统里面，节点组成的网络本来应该是连通的。然而可能因为一些故障，使得有些节点之间不连通了，整个网络就分成了几块区域。数据就散布在了这些不连通的区域中。这就叫分区。当你一个数据项只在一个节点中保存，那么分区出现后，和这个节点不连通的部分就访问不到这个数据了。这时分区就是无法容忍的。提高分区容忍性的办法就是一个数据项复制到多个节点上，那么出现分区之后，这一数据项就可能分布到各个区里。容忍性就提高了。然而，要把数据复制到多个节点，就会带来一致性的问题，就是多个节点上面的数据可能是不一致的。要保证一致，每次写操作就都要等待全部节点写成功，而这等待又会带来可用性的问题。总的来说就是，数据存在的节点越多，分区容忍性越高，但要复制更新的数据就越多，一致性就越难保证。为了保证一致性，更新所有节点数据所需要的时间就越长，可用性就会降低。

​		假如用户的数据在地理上被存储到不同的数据中心,这两个数据中心之间的链接暂时失去了顺序,但是在许多情况下我们任然希望用户能使用网站或者是服务. 这就意味着我们以后要去调和(reconciling)这俩个数据集的分歧,这不仅是一个技术上的挑战也是一个商业风险. 但是他俩都是容易管理的,因此最好提供高可用.

​		一致性和可用性并不是非要选择一个,除非我们把自己限制在强一致性上.但是强一致性仅仅只是一种一致性模型:在这种情况下(the one where you), 您必须放弃可用性，以防止多个数据副本处于活动状态。正如布鲁尔自己所指出的，“三分之二”的解释是有误导性的。

​		如果你从这个讨论中只得到一个想法，那就是:“一致性”,他 不是一个单一的、明确的属性。记住:

​		**[ACID](http://en.wikipedia.org/wiki/ACID) consistency  != [CAP](http://en.wikipedia.org/wiki/CAP_theorem) consistency  !=   [Oatmeal](http://en.wikipedia.org/wiki/Oatmeal) consistency**

​		相反，一致性模型是数据存储为使用它的程序提供的一种保证——任何保证。

> **Consistency model**
>
> 程序员和系统之间的一种约定，其中系统保证，如果程序员遵循某些特定的规则，对数据存储的操作结果将是可预测的

​		C 在 CAP理论中是强一致性, 但是 一致性 却不等价于强一致性.

## 强一致性和其他一致性模型

​		一致性模型可以分为两类,强一致性模型和弱一致性模型

1. 强一致性模型 
   1. 线性一致性
   2. 顺序一致性
2. 弱一致性模型
   1. 客户中心一致性
   2. 因果一致性 可用的最强模型
   3. 最终一致性模

​		强一致性模型保证了更新的明显顺序和可见性等同于(is equivalent to )一个未复制的系统。另一方面，弱一致性模型不做这样的保证。

​		注意，上述不是列举出所有的(exhaustive)模型。同样，一致性模型只是程序员和系统之间的任意(arbitrary)约定，所以它们几乎可以是任何东西。

### 强一致性模型

​		强一致性模型可以更深的划分为俩个近似但是又有些许差异的一致性模型

- 线性一致性: 

  在线性一致性下，所有操作似乎都是按照与全局实时操作顺序一致的顺序原子执行的。(Herlihy & Wing, 1991)

- 顺序一致性

  在顺序一致性下,所有操作都是按照某种顺序原子性的执行,这个顺序与在单个节点上的看到的顺序一致.并且在每个节点上都是一样的.(Lamport, 1979)

​		他们之间关键的区别在于, 线性一致性要求具体的操作起作用的时间与操作的时间是一致的.顺序一致性允许操作进行重新排序只要在每个节点上我们看到的顺序任然是一致的就可以. 唯一的方式我们去区别这俩种模型就是,通过进行系统的所有输入和时间,从客户机与节点交互的角度(perspective)来看，这两者是等价的(equivalent)。

​		差异似乎无关紧要，但值得注意的是，顺序一致性并不构成。

​		强一致性模型允许程序员用分布式的集群来替代一个单结点机器,并且运行不会出现问题.

​		所有的其他一致性模型都会有异常(与保证强一致性的系统相比), 因为他们的行为在某种程度上与没有副本的系统还是有区别的. 但是这些异常也是能接受的,要么是因为我们不关心偶尔出现的问题，要么是因为我们已经编写了以某种方式处理不一致的代码。

​		请注意，对于弱一致性模型并没有任何通用的类型学，因为“不是强一致性模型”(例如。“在某种程度上区别于非复制系统”)可以是几乎任何东西。	

## 客户端中心一致性模型

​		客户端中心是在某种程度上涉及到客户端或者会话概念的一种一致性模型.例如, 客户端中心一致性模型能够保证客户端从不会看到一个数据集旧的版本. 这通常就是通过构建额外的缓存在客户端库中来实现的.因此如果客户端移动到一个备份的只有旧数据的节点, 在使用时客户端库会返回缓存下来的数据而不是直接是使用旧的数据.

​		如果它们所在的复制节点不包含最新版本，客户端可能仍然会看到数据的旧版本.但它们永远不会看到旧版本值重新出现时的异常(例如，因为它们连接到不同的副本)。

## 最终一致性

​		最终一致性认为, 如果你停止改变值, 在一段时间后所有副本都会同步到这个值. 这就意味着在这段时间中以某种不规范的行为访问副本间的数据会不一致. 

​		说某件事最终是一致的，就像说“人最终是死的”。这是一个非常弱的约束，我们可能希望至少对两件事有一些更具体的描述:

​		首先，“最终”是多长?有一个严格的下界会很有用，或者至少知道系统到相同值通常需要多长时间。	

​		第二，副本如何对一个值达成一致?一个总是返回“42”的系统最终是一致的:所有副本都同意相同的值。它只是不会收敛到一个有用的值，因为它只是一直返回相同的固定值。相反，我们想要对方法有一个更好的了解。例如，一种方法是让时间戳最大的值总是获胜。		

​		所以当供应商说“最终的一致性”时，他们指的是一些更精确的术语，比如“最终的最后一个作者获胜，同时读取最新的观察值”的一致性。“如何”很重要，因为一个糟糕的方法可能会导致写丢失——例如，如果一个节点上的时钟设置不正确，并且使用了时间戳。	

​		在关于弱一致性模型的复制方法的章节中更详细地研究这两个问题。

## Further reading

- [Brewer's Conjecture and the Feasibility of Consistent, Available, Partition-Tolerant Web Services](http://lpd.epfl.ch/sgilbert/pubs/BrewersConjecture-SigAct.pdf) - Gilbert & Lynch, 2002
- [Impossibility of distributed consensus with one faulty process](http://scholar.google.com/scholar?q=Impossibility+of+distributed+consensus+with+one+faulty+process) - Fischer, Lynch and Patterson, 1985
- [Perspectives on the CAP Theorem](http://scholar.google.com/scholar?q=Perspectives+on+the+CAP+Theorem) - Gilbert & Lynch, 2012
- [CAP Twelve Years Later: How the "Rules" Have Changed](http://www.infoq.com/articles/cap-twelve-years-later-how-the-rules-have-changed) - Brewer, 2012
- [Uniform consensus is harder than consensus](http://scholar.google.com/scholar?q=Uniform+consensus+is+harder+than+consensus) - Charron-Bost & Schiper, 2000
- [Replicated Data Consistency Explained Through Baseball](http://pages.cs.wisc.edu/~remzi/Classes/739/Papers/Bart/ConsistencyAndBaseballReport.pdf) - Terry, 2011
- [Life Beyond Distributed Transactions: an Apostate's Opinion](http://scholar.google.com/scholar?q=Life+Beyond+Distributed+Transactions%3A+an+Apostate's+Opinion) - Helland, 2007
- [If you have too much data, then 'good enough' is good enough](http://dl.acm.org/citation.cfm?id=1953140) - Helland, 2011
- [Building on Quicksand](http://scholar.google.com/scholar?q=Building+on+Quicksand) - Helland & Campbell, 2009