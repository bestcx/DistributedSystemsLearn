# 	分布式系统四

## 副本

​		复制问题是分布式系统中的众多问题之一。我之所以选择将重点放在这个问题上，而不是其他问题，比如领导人选举、失败检测、互斥、共识和全局快照，是因为这通常是人们最感兴趣的部分。例如，区别并行数据库的一种方式是根据它们的复制特性。此外，复制为许多子问题提供了上下文，如leader选举、故障检测、共识和原子广播。

​		复制是一个组通信问题。什么样的安排和沟通模式给了我们想要的性能和可用性特征?面对网络分区和同时发生的节点故障，我们如何确保容错性、持久性和无分歧?

​		同样，有很多方法可以实现复制。我将在这里采用的方法仅着眼于具有复制的系统可能采用的高级模式。直观地查看这一点有助于将讨论的重点放在整体模式上，而不是所涉及的特定消息传递。我的目标是探索设计空间，而不是解释每个算法的细节。

​		我们先来定义复制是什么样的。我们假设我们有一个初始数据库，客户端发出更改数据库状态的请求。

![replication](http://book.mixu.net/distsys/images/replication-both.png)

这种安排和通信方式可以分为以下几个阶段:

- (Request)客户端向服务器发送请求
- (同步)复制的同步部分发生
- (Response)向客户端返回一个响应
- (Async)复制的异步部分发生

考虑到这些阶段，我们能创造出什么样的交流模式呢?我们所选择的模式的性能和可用性含义是什么?

## 同步复制

第一种模式是同步复制(也称为活动复制、主动复制、推送复制或悲观复制)。让我们画出它的样子:

![replication](http://book.mixu.net/distsys/images/replication-sync.png)

​		在这里，我们可以看到三个不同的阶段:首先，客户端发送请求。接下来，发生我们所说的复制的同步部分。这个术语指的是客户机被阻塞的事实——等待来自系统的应答。

​		在同步阶段，第一个服务器与其他两个服务器联系，并等待，直到收到所有其他服务器的响应。最后，它向客户机发送一个响应，通知结果(例如成功或失败)。

​		所有这些似乎都很简单。如果不讨论同步阶段算法的细节，我们对这种通信模式的具体安排能说些什么呢?首先，注意这是一种 (N - of - N)的方法:在返回响应之前，必须被系统中的每个服务器看到和确认。

​		从性能角度来看，这意味着系统将与其中最慢的服务器一样快。系统也会对网络延迟的变化非常敏感，因为它需要每个服务器在继续之前作出应答。

​		对于N-of-N方法，系统不能容忍任何服务器的丢失。当服务器丢失时，系统无法再向所有节点写入数据，因此无法继续。它可以提供对数据的只读访问，但是在此设计中，不允许在节点失效后进行修改。

​		这种安排可以提供非常强的一致保证:客户机可以确定，当响应返回时，所有N个服务器都已经接收、存储和确认了请求。为了丢失一个可接受的更新，需要丢失所有N个副本，这是你能做的最好的保证。

## 异步复制

​		让我们将其与第二种模式——异步复制(也称为被动复制，或拉复制，或惰性复制)进行对比。正如您可能已经猜到的，这是同步复制的对立面:

![replication](http://book.mixu.net/distsys/images/replication-async.png)

​		在这里，主(/leader / coordinator)立即向客户机发回响应。它最好将更新存储在本地，但是它不会同步执行任何重要工作，并且客户机不会被迫等待服务器之间发生更多轮通信。

​		在稍后的某个阶段，复制任务的异步部分将发生。在这里，主服务器使用某种通信模式联系其他服务器，其他服务器更新它们的数据副本。具体情况取决于所使用的算法。

​		在不深入算法细节的情况下，我们怎么能说这个具体的安排呢?这是一种(1 - of - N)的方法:立即返回响应，稍后再进行更新传播。

​		从性能角度来看，这意味着系统速度很快:客户机不需要花费任何额外的时间来等待系统内部人员完成它们的工作。系统对网络延迟的容忍度也更高，因为内部延迟的波动不会在客户端造成额外的等待。

​		这种安排只能提供较弱的或概率的持久性保证。如果没有出错，数据最终会被复制到所有N台机器上。但是，如果在此之前只丢失了包含数据的服务器，那么数据将永久丢失。

​		对于1-of-N方法，只要至少有一个节点运行，系统就可以保持可用(至少理论上是这样，但实际上负载可能太高)。像这样纯粹的懒惰方法没有提供持久性或一致性保证;您可能被允许向系统写入数据，但不能保证在发生任何错误时可以读回所写入的数据。

​		最后，值得注意的是，被动复制不能确保系统中的所有节点总是包含相同的状态。如果您接受多个位置的写操作，并且不要求这些节点同步同意，那么您将面临分歧的风险:读操作可能从不同的位置返回不同的结果(特别是在节点故障和恢复之后)，并且不能强制执行全局约束(需要与每个节点通信)。

​		我并没有真正提到读(而不是写)期间的通信模式，因为读模式实际上遵循写模式:在读期间，您希望联系尽可能少的节点。我们将在法定人数的背景下进一步讨论这个问题。

​		我们只讨论了两种基本的安排，没有具体的算法。然而，我们已经能够找出很多关于可能的通信模式以及它们的性能、持久性保证和可用性特征的信息。

## 主要复制方法的概述

讨论了两种基本的复制方法:同步复制和异步复制之后，让我们看看主要的复制算法。

很多很多不同的方法可以对复制技术进行分类。我要介绍的第二个区别(在同步和异步之后)是:

- 防止发散的复制方法(单拷贝系统)和
- 存在差异风险的复制方法(多主机系统)

第一组方法的属性是它们“表现得像一个单一系统”。特别是，当部分故障发生时，系统确保只有一个系统副本是活动的。此外，系统确保副本总是一致的。这就是所谓的共识问题。

如果几个进程(或计算机)在某些价值上达成一致，它们就会达成一致。更正式地:

1. 一致性:每个正确的过程必须在相同的价值上达成一致。
2. 完整性:每个正确的过程最多决定一个值，如果它决定某个值，那么它一定是由某个过程提出的。
3. 终止:所有进程最终会达成一个决策。
4. 有效性:如果所有正确的过程提出相同的值V，那么所有正确的过程决定V。

互斥、领袖选举、多播和原子广播都是更普遍的共识问题的例子。维持单副本一致性的复制系统需要以某种方式解决一致性问题。

保持单副本一致性的复制算法包括:

1n message(异步主/备份)
2n message(同步主/备份)
4n message(两阶段提交，多paxos)
6n message(3阶段提交，Paxos与反复的领导人选举)

​		这些算法在容错方面各不相同(例如它们可以容错的类型)。我只是根据执行算法期间交换的消息数量对它们进行了分类，因为我认为，尝试寻找“我们添加的消息交换能买到什么”这个问题的答案是很有趣的。

​		下面的图表改编自谷歌的Ryan Barret，描述了不同选择的一些方面:

![Comparison of replication methods, from http://www.google.com/events/io/2009/sessions/TransactionsAcrossDatacenters.html](http://book.mixu.net/distsys/images/google-transact09.png)

​		上图中的一致性、延迟、吞吐量、数据丢失和故障转移特征实际上可以追溯到两种不同的复制方法:同步复制(例如在响应之前等待)和异步复制。当你等待时，你的性能会变差，但一致性会更强。当我们讨论分区(和延迟)容忍时，2PC和仲裁系统之间的吞吐量差异将变得明显。

​		在这张图中，强制执行弱(/最终)一致性的算法被归为一类(“gossip”)。但是，我将更详细地讨论用于弱一致性的复制方法——闲谈和(部分)仲裁系统。“事务”行实际上更多地指全局谓词求值，在具有弱一致性的系统中不支持全局谓词求值(尽管可以支持本地谓词求值)。

> Gossip protocol 也叫 Epidemic Protocol （流行病协议），实际上它还有很多别名，比如：“流言算法”、“疫情传播算法”等。
>
> gossip 就是一个并行的图广度优先遍历算法。假设A得到某些信息，更新了自身的信息，A需要将信息告诉B、C等，然后B、C告诉其他的D、E、F、G，一直遍历。如果节点B收到A的消息，发现自己早就知道这个消息就直接忽略，从而可以防止图重复遍历。和路由器使用的路由表同步算法类似，只不过路由器还会维护每个路径的cost。gossip常用于维护集群的拓扑结构（路由器也是），被redis、consul使用。

​		值得注意的是，强制执行弱一致性需求的系统具有更少的泛型算法，以及更多可以选择性应用的技术。由于不强制单一副本一致性的系统可以像由多个节点组成的分布式系统一样自由地运行，因此需要修正的明显目标较少，重点更多地放在给人们提供一种方法来推理他们所拥有的系统的特征上。

​		例如:

- 以客户为中心的一致性模型试图在允许分歧的同时提供更容易理解的一致性保证。
- crdt(收敛和交换复制数据类型)利用某些状态和基于操作的数据类型的半格属性(结合性、交换性、等幂)。
- 汇流分析(如在Bloom语言中)使用有关计算单调性的信息来最大限度地利用无序性。
- PBS(概率有限陈旧)使用从真实系统中收集的模拟和信息来描述部分仲裁系统的预期行为。

首先，我将进一步讨论所有这些;让我们看看保持单副本一致性的复制算法。

## 主从复制

​		主/备份复制(也称为主从复制或日志传送)可能是最常用的复制方法，也是最基本的算法。所有更新都在主服务器上执行，操作(或更改)日志通过网络发送到备份副本。有两种变体:

- 异步主从复制

- 同步主从复制

同步版本需要两条消息(“更新”+“确认接收”)，而异步版本可以只运行一条消息(“更新”)。

​		P/B很常见。例如，默认情况下MySQL复制使用异步变体。MongoDB也使用P/B(附加一些故障转移过程)。所有操作都在一个主服务器上执行，主服务器将它们序列化到本地日志，然后将本地日志异步复制到备份服务器。

​		正如我们前面在异步复制上下文中讨论的那样，任何异步复制算法都只能提供弱持久性保证。在MySQL复制中，这表现为复制延迟:异步备份总是至少比主备份晚一个操作。如果主备份失败，那么尚未发送到备份的更新将丢失。

​		主/备份复制的同步变体确保在返回到客户端之前，写操作已经存储在其他节点上——以等待其他副本的响应为代价。然而，值得注意的是，即使是这种变体也只能提供较弱的担保。考虑以下简单的故障场景:

1. 主端接收一个写操作并将其发送给备份端

2. 备份持续并ack写入

3. 然后primary在向客户端发送ACK之前失败

   ​		

​        客户端现在假设提交失败，但是备份提交了它;如果将备份提升为主备份，则将是不正确的。可能需要手动清理以协调失败的主备份或分歧备份。

​		我在简化这里。虽然所有主/备份复制算法都遵循相同的通用消息传递模式，但它们在故障转移的处理、副本长时间脱机等方面有所不同。然而，在这个方案中，对于主节点的意外故障是不可能有弹性的。

​		基于日志传送/主/备份的方案的关键是，它们只能提供最好的保证(例如，如果节点在不合适的时间失败，它们很容易丢失更新或不正确的更新)。此外，P/B方案很容易出现脑裂，即由于临时网络问题而将故障转移到备份，并导致主备份同时处于活动状态。

​		防止不恰当的失败导致一致性保证被违反;我们需要添加另一轮消息传递，这将使我们获得两阶段提交协议(2PC)。

## 2阶段提交(2PC)

​		两阶段提交(2PC)是许多经典关系数据库中使用的协议。例如，MySQL集群(不要与常规的MySQL混淆)使用2PC提供同步复制。下图说明了消息流:

```

[ Coordinator ] -> OK to commit?     [ Peers ]
                <- Yes / No

[ Coordinator ] -> Commit / Rollback [ Peers ]
                <- ACK
                
```

​		在第一阶段(投票)中，协调器将更新发送给所有参与者。每个参与者处理更新并投票决定是提交还是中止。在投票提交时，参与者将更新存储到一个临时区域(预写日志)。在第二阶段完成之前，更新被认为是临时的。

​		在第二阶段(决策)，协调者决定结果并通知每个参与者。如果所有参与者都投票决定提交，那么更新将从临时区域中取出并永久保存。

​		在提交被认为是永久提交之前设置第二个阶段是有用的，因为它允许系统在节点失败时回滚更新。相反，在主/备份(“1PC”)中，没有回滚在某些节点上失败而在其他节点上成功的操作的步骤，因此副本可能会出现分歧。

​		2PC容易阻塞，因为单个节点故障(参与者或协调者)阻塞进程，直到节点恢复。由于第二个阶段，恢复通常是可能的，在这个阶段中，其他节点将被告知系统状态。请注意，2PC假定每个节点上的稳定存储中的数据永远不会丢失，并且没有节点永远崩溃。如果稳定存储中的数据在崩溃中损坏，数据丢失仍然是可能的。

​		节点故障期间恢复过程的细节非常复杂，所以我就不细说了。主要任务是确保对磁盘的写入是持久的(例如，刷新到磁盘而不是缓存)，并确保做出了正确的恢复决策(例如，了解这轮更新的结果，然后在本地重新执行或撤销更新)。

​		正如我们在关于CAP的章节中学到的，2PC是一个CA -它不是分区宽容。2PC地址的故障模型不包括网络分区;指定的从节点故障中恢复的方法是等待网络分区恢复。如果一名协调员失败了，没有安全的方法来提拔新的协调员;相反，需要人工干预。2PC对延迟也相当敏感，因为它是一种N-of-N的写方法，在最慢的节点确认后才能进行写操作。

​		2PC在性能和容错之间取得了良好的平衡，这就是它在关系数据库中流行的原因。然而，较新的系统通常使用分区容忍共识算法，因为这种算法可以从临时网络分区提供自动恢复，并更优雅地处理节点之间延迟的增加。

接下来让我们看看分区容错共识算法。

## 分区容忍一致性算法

​		分区容错共识算法是我们所要做的保持单副本一致性的容错算法。还有一类容错算法:可以容忍任意(拜占庭式)错误的算法;这些包括通过恶意操作而导致故障的节点。这类算法很少在商业系统中使用，因为它们运行起来更昂贵，实现起来更复杂——因此我将不讨论它们。

​		在分区容错共识算法方面，最著名的算法是Paxos算法。然而，它的执行和解释是出了名的困难，所以我将把重点放在Raft上，这是一个最近(2013年初)设计得更容易教和执行的算法。让我们首先看看网络分区和分区容错共识算法的一般特征。

### 什么是网络分区

​		网络分区是指连接到一个或几个节点的网络链路出现故障。节点本身继续保持活动状态，它们甚至能够接收来自网络分区中它们这一侧的客户端的请求。正如我们早些时候在讨论CAP定理时所了解到的那样，确实会发生网络分区，并不是所有系统都能优雅地处理它们。

​		网络分区很棘手，因为在网络分区期间，不可能区分失败的远程节点和不可达的节点。如果出现了一个网络分区，但没有节点故障，那么系统将被划分为两个同时活动的分区。下面的两个图说明了网络分区如何与节点故障相似。

​		2个节点的系统，一个故障 vs.一个网络分区:

![replication](http://book.mixu.net/distsys/images/system-of-2.png)

​		3个节点的系统，一个故障 vs.一个网络分区:

![replication](http://book.mixu.net/distsys/images/system-of-3.png)

一个强制单一副本一致性的系统必须有某种方法来打破对称性:否则，它将分裂成两个独立的系统，它们可能彼此分离，并不能再维持单一副本的幻觉。

强制执行单副本一致性的系统的网络分区容忍要求在网络分区期间，只有一个系统分区是活动的，因为在网络分区期间不可能防止分歧(例如CAP定理)。

## 多数决定

​		这就是为什么划分宽容共识算法依赖于多数投票的原因。要求大多数节点-而不是所有节点(如2PC) -同意更新允许少数节点宕机，或缓慢，或由于网络分区不可达。只要N个节点中的(N/2 + 1)个节点是可用的，系统就可以继续运行。

​		分区容忍共识算法使用奇数个节点(例如3、5或7)。如果只有两个节点，就不可能在故障后获得明显的多数。例如，如果节点数量是3，那么系统对于一个节点故障是有弹性的;对于5个节点，系统对于2个节点故障具有弹性。

​		当出现网络分区时，分区的行为是非对称的。一个分区将包含大部分节点。少数分区将停止处理操作，以防止网络分区期间出现分歧，但多数分区可以保持活动。这确保了只有一个系统状态的副本保持活动状态。

​		多数也是有用的，因为它们可以容忍分歧:如果出现扰动或故障，节点可能会以不同的方式投票。然而，由于只能有一个多数决定，暂时的分歧最多可以阻止协议进行(放弃活性)，但它不能违反单副本一致性标准(安全性)。

### 角色

构建系统有两种方式:所有节点可能具有相同的职责，或者节点可能具有独立、不同的角色。

用于复制的共识算法通常为每个节点选择不同的角色。拥有一个固定的主服务器是一种优化，它使系统更有效率，因为我们知道所有请注意，拥有不同的角色并不会阻止系统从领导者(或任何其他角色)的失败中恢复。仅仅因为角色在正常操作期间是固定的，并不意味着一个人不能通过在失败后重新分配角色从失败中恢复过来(例如通过领导选举阶段)。节点可以重用leader选举的结果，直到节点故障和/或网络分区发生。

Paxos和Raft都使用了不同的节点角色。特别是，它们有一个领导节点(Paxos中的“提议者”)，负责在正常操作期间进行协调。在正常操作中，其余节点是追随者(Paxos中的“接受者”或“投票者”)。的更新都必须通过该服务器。不是leader的节点只需要将它们的请求转发给leader

### 阶段

​		在Paxos和Raft中，每个正常运行的时期都被称为一个阶段(Raft中的“term”)。在每个纪元中，只有一个节点是指定的领袖(类似的系统在日本使用，年号随着皇位继承而改变)。

![replication](http://book.mixu.net/distsys/images/epoch.png)

​		在一次成功的选举之后，同样的领导人协调直到时代的结束。如上图所示(来自Raft论文)，一些选举可能会失败，导致时代立即结束。

​		epoch作为一个逻辑时钟，允许其他节点识别过时的节点何时开始通信——被分区或退出操作的节点的epoch号将小于当前节点，并且它们的命令将被忽略。

### 斗争选举

​		在正常的操作中，一个分区容忍共识算法是相当简单的。正如我们前面看到的，如果我们不关心容错，我们可以只使用2PC。大部分的复杂性来自于确保一旦达成共识，它不会丢失，并且协议能够处理由于网络或节点故障导致的leader变更。

​		所有节点都作为追随者启动;一个节点在开始时被选为leader。在正常操作期间，leader维护一个心跳，允许follower检测leader是否故障或被分区。

​		当节点检测到leader已经没有响应了(或者在初始情况下，leader不存在)，它会切换到一个中间状态(在Raft中称为“candidate”)，在这个状态下，它会将term/epoch值增加1，发起leader选举并竞争成为新的leader。

​		为了被选为领导，节点必须获得多数选票。分配选票的一种方式是简单地按照先到先得的原则分配;这样，最终会选出一位领导人。在两次尝试当选之间增加随机的等待时间将减少同时尝试当选的节点数量。

​		在每个阶段，领导人每次提出一个价值，供投票表决。在每个阶段，每个提案都用一个严格递增的唯一数字编号。追随者(投票者/接受者)接受他们收到的关于特定提案号的第一个提案。

​		正常运行时，所有提案都要经过leader节点。当客户端提交一个提议(例如更新操作)时，leader会联系仲裁中的所有节点。如果没有竞争性的提议存在(基于追随者的响应)，领导者提出价值。如果大多数追随者接受这个值，那么这个值就被认为是被接受的。

​		由于另一个节点也可能试图充当leader，我们需要确保一旦一个提议被接受，它的值永远不会改变。否则，一项已经被接受的提议，例如可能会被竞争领导人收回。Lamport指出:

> 如果选择了一个值为v的提议，那么每个被选择的更高编号的提议都有值v。

​		为了确保该属性的正确性，追随者和提议者都必须受到算法的约束，不得更改已被大多数人接受的值。注意，“这个值永远不会改变”指的是协议的单个执行(或run / instance / decision)的值。一个典型的复制算法将运行该算法的多次执行，但是为了保持简单，对该算法的大多数讨论都集中在一次运行上。我们想要防止决策历史被改变或覆盖。

​		为了执行此属性，提议者必须首先向追随者询问他们(最高编号)接受的提议和值。如果提议者发现提议已经存在，那么它必须简单地完成协议的执行，而不是提出自己的提议。Lamport指出:

> 如果选择了一个值为v的提议，那么任何提议者发出的每一个更高编号的提议都有值v。

​		对于任意v和n，如果一个值为v且编号为n的提议被[领导者]发出，那么存在一个由大多数接受者[追随者]组成的集合S，使得(a) S中没有一个接受者接受任何编号小于n的提议，或(b) v为S中追随者接受的所有小于n的提案中编号最高的提案的值。

​		这是Paxos算法及其衍生算法的核心。直到协议的第二阶段，才会选择建议的值。投保人有时必须简单地重新传送先前做出的决定以确保安全(如P2c条款b)，直到他们知道他们可以自由地强加自己的建议价值(如条款a)。

​		如果存在多个以前的提案，则建议编号最高的提案值。只有在没有任何竞争性提案的情况下，提案方才可能试图强加自己的价值。

​		为了确保在提议者向每个接受者询问其最近的值之间没有竞争性的提议出现，提议者要求追随者不要接受比当前提议数更低的提议。

​		利用Paxos将碎片拼凑起来，做出决定需要两轮沟通:

```
[ Proposer ] -> Prepare(n)                                [ Followers ]
             <- Promise(n; previous proposal number
                and previous value if accepted a
                proposal in the past)

[ Proposer ] -> AcceptRequest(n, own value or the value   [ Followers ]
                associated with the highest proposal number
                reported by the followers)
                <- Accepted(n, value)
```

​		准备阶段允许提案人了解任何竞争对手或以前的提案。第二个阶段是提出一个新值或先前接受的值。在某些情况下-例如两个提议者同时活跃(决斗);如果消息丢失;或者，如果大多数节点都失败了，那么大多数人不会接受任何提议。但这是可以接受的，因为建议值的决策规则收敛于一个单一值(在前面的尝试中，建议数最高的值)。

​		事实上，根据FLP不可能结果，这是我们能做的最好的:当关于消息传递界限的保证不成立时，解决共识问题的算法必须要么放弃安全性，要么放弃生命力。Paxos放弃了活力:它可能不得不无限期地推迟决策，直到某个时间点没有竞争的领导者，并且大多数节点接受一个提议。这比违反安全保证更可取。

​		当然，实现这个算法比听起来要困难得多。即使在专家手中，这些小问题加起来也会构成相当可观的代码量。这些问题包括:

- 实际的优化:
  - 通过领导租赁(而不是心跳)避免反复的领导人选举
  - 在领导者身份不变的稳定状态下，避免重复的提议信息
- 确保追随者和提议者不会丢失稳定存储中的项，并且存储在稳定存储中的结果没有轻微损坏(例如磁盘损坏)。
- 使集群成员以一种安全的方式改变(例如，base Paxos依赖于多数成员总是相交于一个节点的事实，如果成员可以任意改变，这就不成立)
- 在崩溃、磁盘丢失或提供新节点后，以安全有效的方式更新新副本的过程
- 在一段合理的时间后进行快照和垃圾收集以保证安全所需的数据的过程(例如平衡存储要求和容错要求)

谷歌的Paxos Made Live论文详细阐述了其中的一些挑战。



## 分区容忍共识算法:Paxos, Raft, ZAB

​		希望这已经让您了解了分区宽容共识算法是如何工作的。我鼓励您阅读进一步阅读部分中的一篇文章，以掌握不同算法的细节。

​		Paxos。Paxos是编写强一致性分区容错复制系统时最重要的算法之一。它被用于谷歌的许多系统中，包括BigTable/Megastore使用的Chubby锁管理器、谷歌文件系统以及Spanner。

​		Paxos以希腊的Paxos岛命名，最初是由Leslie Lamport在1998年的一篇名为《兼职议会》的论文中提出的。它通常被认为是难以实现的，已经有一系列来自具有相当多分布式系统专业知识的公司的论文解释了进一步的实际细节(参见进一步阅读)。你可能想在这里和这里阅读Lamport对这个问题的评论。

​		这些问题大多与Paxos被描述为单轮共识决策这一事实有关，但实际的工作执行通常希望有效地运行多轮共识。这导致了核心协议上的许多扩展的开发，任何对构建基于paxos的系统感兴趣的人都需要消化这些扩展。此外，还有其他实际挑战，比如如何促进集群成员的更改。

​		ZAB。ZAB -在Apache Zookeeper中使用的Zookeeper原子广播协议。Zookeeper是一个为分布式系统提供协调原语的系统，被许多以hadoop为中心的分布式系统用于协调(例如HBase、Storm、Kafka)。Zookeeper基本上是开源社区的Chubby版本。从技术上讲，原子广播是一个不同于纯粹共识的问题，但它仍然属于确保强一致性的分区容错算法的范畴。

​		Raft。Raft是最近(2013年)加入到这个算法家族中的一个。它被设计成比Paxos更容易教，同时提供同样的保证。特别的是，该算法的不同部分更加清晰的分离，文中还描述了一种聚类成员变化的机制。受ZooKeeper的启发，它最近被etcd所采用。

### 一致性强的复制方法

在本章中，我们研究了强制强一致性的复制方法。从同步工作和异步工作的对比开始，我们逐步提高了能够容忍日益复杂的故障的算法。以下是每个算法的一些关键特征:

- 主/备份
  单一的、静态的主
  复制的日志，奴隶不涉及执行操作
  复制延迟没有限制
  不分区宽容
  手动/特别故障转移，而不是容错，“热备份”
- 2 pc
  全票通过:要么提交，要么放弃
  静态的主
  在提交期间，如果协调器和节点同时发生故障，2PC将无法存活
  不允许分区，尾巴延迟敏感
- Paxos
  多数投票
  动态主
  作为协议的一部分，健壮的n/2-1并发故障
  对尾部延迟不太敏感

## 拓展阅读

#### Primary-backup and 2PC

- [Replication techniques for availability](http://scholar.google.com/scholar?q=Replication+techniques+for+availability) - Robbert van Renesse & Rachid Guerraoui, 2010
- [Concurrency Control and Recovery in Database Systems](http://research.microsoft.com/en-us/people/philbe/ccontrol.aspx)

#### Paxos

- [The Part-Time Parliament](http://research.microsoft.com/users/lamport/pubs/lamport-paxos.pdf) - Leslie Lamport
- [Paxos Made Simple](http://research.microsoft.com/users/lamport/pubs/paxos-simple.pdf) - Leslie Lamport, 2001
- [Paxos Made Live - An Engineering Perspective](http://research.google.com/archive/paxos_made_live.html) - Chandra et al
- [Paxos Made Practical](http://scholar.google.com/scholar?q=Paxos+Made+Practical) - Mazieres, 2007
- [Revisiting the Paxos Algorithm](http://groups.csail.mit.edu/tds/paxos.html) - Lynch et al
- [How to build a highly available system with consensus](http://research.microsoft.com/lampson/58-Consensus/Acrobat.pdf) - Butler Lampson
- [Reconfiguring a State Machine](http://research.microsoft.com/en-us/um/people/lamport/pubs/reconfiguration-tutorial.pdf) - Lamport et al - changing cluster membership
- [Implementing Fault-Tolerant Services Using the State Machine Approach: a Tutorial](http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.20.4762) - Fred Schneider

#### Raft and ZAB

- [In Search of an Understandable Consensus Algorithm](https://ramcloud.stanford.edu/wiki/download/attachments/11370504/raft.pdf), Diego Ongaro, John Ousterhout, 2013
- [Raft Lecture - User Study](http://www.youtube.com/watch?v=YbZ3zDzDnrw)
- [A simple totally ordered broadcast protocol](http://labs.yahoo.com/publication/a-simple-totally-ordered-broadcast-protocol/) - Junqueira, Reed, 2008
- [ZooKeeper Atomic Broadcast](http://labs.yahoo.com/publication/zab-high-performance-broadcast-for-primary-backup-systems/) - Reed, 2011
